{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-15 17:44:19,680 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n",
      "2020-11-15 17:44:19,682 INFO sqlalchemy.engine.base.Engine ()\n",
      "2020-11-15 17:44:19,684 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n",
      "2020-11-15 17:44:19,685 INFO sqlalchemy.engine.base.Engine ()\n",
      "2020-11-15 17:44:19,689 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"lerua\")\n",
      "2020-11-15 17:44:19,691 INFO sqlalchemy.engine.base.Engine ()\n",
      "2020-11-15 17:44:19,694 INFO sqlalchemy.engine.base.Engine PRAGMA temp.table_info(\"lerua\")\n",
      "2020-11-15 17:44:19,696 INFO sqlalchemy.engine.base.Engine ()\n",
      "2020-11-15 17:44:19,698 INFO sqlalchemy.engine.base.Engine \n",
      "CREATE TABLE lerua (\n",
      "\tid INTEGER NOT NULL, \n",
      "\ttitle VARCHAR(150) NOT NULL, \n",
      "\tlink VARCHAR(500) NOT NULL, \n",
      "\tprice INTEGER, \n",
      "\tphotos VARCHAR(500) NOT NULL, \n",
      "\tfeature TEXT NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2020-11-15 17:44:19,699 INFO sqlalchemy.engine.base.Engine ()\n",
      "2020-11-15 17:44:19,716 INFO sqlalchemy.engine.base.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import Column, ForeignKey, Integer, String, UniqueConstraint, Text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "class Lerua(Base):\n",
    "    __tablename__ = 'lerua'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    title = Column(String(200), nullable=False)\n",
    "    link = Column(String(400), nullable=False)\n",
    "    price = Column(Integer, nullable=True)\n",
    "    photos = Column(String(400), nullable=False)\n",
    "    feature = Column(Text, nullable=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    engine = create_engine('sqlite:///leruaDB.db', echo=True)\n",
    "    Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_NAME = 'lerua'\n",
    "\n",
    "SPIDER_MODULES = ['lerua.spider']\n",
    "NEWSPIDER_MODULE = 'lerua.spider'\n",
    "\n",
    "IMAGES_STORE = 'photo'\n",
    "\n",
    "LOG_ENABLED = True\n",
    "LOG_LEVEL = 'DEBUG'\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 YaBrowser/20.9.3.126 Yowser/2.5 Safari/537.36'\n",
    "\n",
    "\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "\n",
    "CONCURRENT_REQUESTS = 8\n",
    "\n",
    "DOWNLOAD_DELAY = 2\n",
    "\n",
    "COOKIES_ENABLED = True\n",
    "\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "   'lerua.pipelines.DataBasePipeline': 300,\n",
    "   'lerua.pipelines.LeruaPhotosPipeline': 200,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DataE\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: ScrapyDeprecationWarning: scrapy.loader.processors.TakeFirst is deprecated, instantiate itemloaders.processors.TakeFirst instead.\n",
      "C:\\DataE\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: ScrapyDeprecationWarning: scrapy.loader.processors.TakeFirst is deprecated, instantiate itemloaders.processors.TakeFirst instead.\n",
      "C:\\DataE\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: ScrapyDeprecationWarning: scrapy.loader.processors.TakeFirst is deprecated, instantiate itemloaders.processors.TakeFirst instead.\n",
      "C:\\DataE\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: ScrapyDeprecationWarning: scrapy.loader.processors.MapCompose is deprecated, instantiate itemloaders.processors.MapCompose instead.\n",
      "C:\\DataE\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: ScrapyDeprecationWarning: scrapy.loader.processors.MapCompose is deprecated, instantiate itemloaders.processors.MapCompose instead.\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.loader.processors import MapCompose, TakeFirst\n",
    "\n",
    "\n",
    "def int_price(value):\n",
    "    return int(value.replace(' ', ''))\n",
    "\n",
    "\n",
    "def f_preproc(value):\n",
    "    #print(value.xpath('./div'))\n",
    "    f_dict = {}\n",
    "    for f in value.xpath('./div'):\n",
    "        key = f.xpath('.//dt/text()').extract_first()\n",
    "        value = f.xpath('.//dd/text()').extract_first().replace('\\n', '').replace(' ', '')\n",
    "\n",
    "        f_dict[key] = value\n",
    "    return f_dict\n",
    "\n",
    "\n",
    "class LeruaItem(scrapy.Item):\n",
    "    title = scrapy.Field(output_processor=TakeFirst())\n",
    "    link = scrapy.Field(output_processor=TakeFirst())\n",
    "    price = scrapy.Field(output_processor=TakeFirst(), input_processor=MapCompose(int_price))\n",
    "    photo_links = scrapy.Field()\n",
    "    feature = scrapy.Field(input_processor=MapCompose(f_preproc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeruaSpiderMiddleware:\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class LeruaDownloaderMiddleware:\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeroymerlinSpider(scrapy.Spider):\n",
    "    name = 'leroymerlin'\n",
    "    allowed_domains = ['leroymerlin.ru']\n",
    "\n",
    "    def __init__(self, search):\n",
    "        self.start_urls = [f'https://leroymerlin.ru/search/?q={search}']\n",
    "\n",
    "    def parse(self, response):\n",
    "        next_page = response.xpath(\"//a[@class='paginator-button next-paginator-button']/@href\").extract_first()\n",
    "        goods_links = response.xpath('//a[@class=\"plp-item__info__title\"]')\n",
    "        for link in goods_links:\n",
    "            yield response.follow(link, callback=self.parse_goods)\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "\n",
    "    def parse_goods(self, response: HtmlResponse):\n",
    "        loader = ItemLoader(item=LeruaItem(), response=response)\n",
    "        loader.add_xpath('title', '//h1[@class=\"header-2\"]/text()')\n",
    "        loader.add_value('link', response.url)\n",
    "        loader.add_xpath('price', '//span[@slot=\"price\"]/text()')\n",
    "        loader.add_xpath('photo_links', '//uc-pdp-media-carousel[@slot=\"media-content\"]/picture/source[1]/@data-origin')\n",
    "        loader.add_value('feature', response.xpath('//uc-pdp-section-vlimited/dl'))\n",
    "        yield loader.load_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataBasePipeline:\n",
    "    def __init__(self):\n",
    "        engine = create_engine('sqlite:///leruaDB.db', echo=True)\n",
    "        Base.metadata.bind = engine\n",
    "        DBSession = sessionmaker(bind=engine)\n",
    "        self.session = DBSession()\n",
    "    def process_item(self, item, spider):\n",
    "        new_vacancy = Lerua(\n",
    "            title=item['title'],\n",
    "            link=item['link'],\n",
    "            price=item['price'],\n",
    "            photos=r'/photo/' + item['title'],\n",
    "            feature=str(item['feature'])\n",
    "        )\n",
    "        try:\n",
    "            self.session.add(new_vacancy)\n",
    "            self.session.commit()\n",
    "        except IntegrityError:\n",
    "            print('Проблема с загрузкой Данных!')\n",
    "            self.session.rollback()\n",
    "        return item\n",
    "\n",
    "    def __del__(self):\n",
    "        self.session.close()\n",
    "\n",
    "\n",
    "class LeruaPhotosPipeline(ImagesPipeline):\n",
    "    def get_media_requests(self, item, info):\n",
    "        print('_____', item)\n",
    "        if item['photo_links']:\n",
    "            for img_link in item['photo_links']:\n",
    "                try:\n",
    "                    yield scrapy.Request(img_link, meta=item)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "    def file_path(self, request, response=None, info=None):\n",
    "        item = request.meta\n",
    "        name = request.url.split('/')[-1]\n",
    "        return f\"/{item['title']}/{name}\"\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        if results:\n",
    "            item['photo_links'] = [itm[1] for itm in results if itm[0]]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'settings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-680e1a1800cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcrawler_settings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSettings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mcrawler_settings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawlerProcess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrawler_settings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'settings' is not defined"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.settings import Settings\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler_settings = Settings()\n",
    "    crawler_settings.setmodule(settings)\n",
    "\n",
    "    process = CrawlerProcess(settings=crawler_settings)\n",
    "    process.crawl(LeroymerlinSpider, search='диван')\n",
    "\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
