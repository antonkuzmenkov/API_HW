{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.http import HtmlResponse\n",
    "from instaparser.items import InstaparserItem\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urlencode\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class InstagramSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'instagram'\n",
    "    allowed_domains = ['instagram.com']\n",
    "    start_urls = ['http://instagram.com/']\n",
    "\n",
    "    insta_login_link = 'https://www.instagram.com/accounts/login/ajax/'\n",
    "    graphql_url = 'https://www.instagram.com/graphql/query/'\n",
    "    followers_hash = 'c76146de99bb02f6415203be841dd25a'\n",
    "    followings_hash = 'd04b0a864b4b54837c0d870b0e77e076'\n",
    "\n",
    "    insta_login = 'логин'\n",
    "    insta_psw = 'пароль'\n",
    "    target_accounts = ['ai_machine_learning', 'omgames_blog']  # список (list) целевых аккаунтов, которые будем парсить\n",
    "\n",
    "    def parse(self, response: HtmlResponse):\n",
    "            csrf_token = self.fetch_csrf_token(response.text)  # получаем csrf\n",
    "            yield scrapy.FormRequest(                         #авторизуемся\n",
    "                url=self.insta_login_link,\n",
    "                method='POST',\n",
    "                callback=self.login,\n",
    "                formdata={'username': self.insta_login, 'enc_password': self.insta_psw},\n",
    "                headers={'X-CSRFToken': csrf_token}\n",
    "            )\n",
    "\n",
    "    def login(self, response: HtmlResponse):\n",
    "        j_body = json.loads(response.text)\n",
    "        if j_body['authenticated']:\n",
    "            for account in self.target_accounts:    #переходим на каждую страницу целевых аккаунтов для парсинга\n",
    "                yield response.follow(\n",
    "                    f'/{account}',\n",
    "                    callback=self.target_user_parse,\n",
    "                    cb_kwargs={'target_username': account}\n",
    "                )\n",
    "\n",
    "    def target_user_parse(self, response: HtmlResponse, target_username):\n",
    "        target_user_id = self.fetch_user_id(response.text, target_username)\n",
    "        variables = {'id': target_user_id,\n",
    "                     'first': 24\n",
    "                     }\n",
    "\n",
    "        url_followers = f'{self.graphql_url}?query_hash={self.followers_hash}&{urlencode(variables)}'\n",
    "        yield response.follow(\n",
    "            url_followers,\n",
    "            callback=self.users_parse,\n",
    "            cb_kwargs={'target_username': target_username, 'flag': 'followers',  'variables': deepcopy(variables)}\n",
    "        )\n",
    "\n",
    "        url_followings = f'{self.graphql_url}?query_hash={self.followings_hash}&{urlencode(variables)}'\n",
    "        yield response.follow(\n",
    "            url_followings,\n",
    "            callback=self.users_parse,\n",
    "            cb_kwargs={'target_username': target_username, 'flag': 'followings', 'variables': deepcopy(variables)}\n",
    "        )\n",
    "\n",
    "    def users_parse(self, response: HtmlResponse, target_username, flag, variables):\n",
    "        j_data = json.loads(response.text)\n",
    "        type_field = 'edge_followed_by' if flag == 'followers' else 'edge_follow'\n",
    "        page_info = j_data.get('data').get('user').get(type_field).get('page_info')\n",
    "        if page_info['has_next_page']:\n",
    "            variables['after'] = page_info['end_cursor']\n",
    "\n",
    "            url = f\"{response.url[:response.url.find('&')]}&{urlencode(variables)}\"\n",
    "            yield response.follow(\n",
    "                url,\n",
    "                callback=self.users_parse,\n",
    "                cb_kwargs={'target_username': target_username, 'flag': flag, 'variables': deepcopy(variables)}\n",
    "            )\n",
    "            users = j_data.get('data').get('user').get(type_field).get('edges')\n",
    "            for user in users:\n",
    "                node = user.get('node')\n",
    "                item = InstaparserItem(\n",
    "                    _id=node.get('id'),\n",
    "                    user_name=node.get('username'),\n",
    "                    full_name=node.get('full_name'),\n",
    "                    photo=node.get('profile_pic_url'),\n",
    "                    insert_to_collection=f'{target_username}_{flag}'\n",
    "                )\n",
    "                yield item\n",
    "\n",
    "    def fetch_csrf_token(self, text):\n",
    "        matched = re.search('\\\"csrf_token\\\":\\\"\\\\w+\\\"', text).group()\n",
    "        return matched.split(':').pop().replace(r'\"', '')\n",
    "    def fetch_user_id(self, text, username):\n",
    "        matched = re.search(\n",
    "            '{\\\"id\\\":\\\"\\\\d+\\\",\\\"username\\\":\\\"%s\\\"}' % username, text\n",
    "        ).group()\n",
    "        return json.loads(matched).get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class InstaparserItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    _id = scrapy.Field()\n",
    "    user_name = scrapy.Field()\n",
    "    full_name = scrapy.Field()\n",
    "    photo = scrapy.Field()\n",
    "    insert_to_collection = scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import signals\n",
    "\n",
    "from itemadapter import is_item, ItemAdapter\n",
    "\n",
    "\n",
    "class InstaparserSpiderMiddleware:\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class InstaparserDownloaderMiddleware:\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.pipelines.images import ImagesPipeline\n",
    "import scrapy\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class InstagramPhotosPipeline(ImagesPipeline):\n",
    "\n",
    "    def get_media_requests(self, item, info):\n",
    "        if item['photo']:\n",
    "            try:\n",
    "                yield scrapy.Request(item['photo'], meta=item)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    def item_completed(self, results, item, info):\n",
    "        if results:\n",
    "            item['photo'] = results[0][1] if results[0][0] else None\n",
    "        return item\n",
    "\n",
    "\n",
    "class DataBasePipeline:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = MongoClient('localhost', 27017)\n",
    "        self.mongodb = self.client.instagram\n",
    "\n",
    "    def __del__(self):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "\n",
    "        collection = item['insert_to_collection']\n",
    "        del item['insert_to_collection']\n",
    "\n",
    "        if self.mongodb[collection].count_documents({'_id': item['_id']}) == 0:\n",
    "            self.mongodb[collection].insert_one(item)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "#  подписан пользователь\n",
    "def get_followers(name:str):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.instagram\n",
    "    profiles = db[name + '_followers']\n",
    "    for profile in profiles:\n",
    "        print(profile)\n",
    "\n",
    "# список подписчиков указанного пользователя\n",
    "def get_followings(name:str):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.instagram\n",
    "    profiles = db[name + '_followings']\n",
    "    for profile in profiles:\n",
    "        print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.settings import Settings\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    crawler_settings = Settings()\n",
    "    crawler_settings.setmodule(settings)\n",
    "    process = CrawlerProcess(settings=crawler_settings)\n",
    "    process.crawl(InstagramSpider)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT_NAME = 'instaparser'\n",
    "\n",
    "IMAGES_STORE = 'photo'\n",
    "\n",
    "SPIDER_MODULES = ['instaparser.spiders']\n",
    "NEWSPIDER_MODULE = 'instaparser.spiders'\n",
    "\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "LOG_ENABLED = True\n",
    "LOG_LEVEL = 'DEBUG'\n",
    "\n",
    "CONCURRENT_REQUESTS = 16\n",
    "\n",
    "DOWNLOAD_DELAY = 1.25\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'instaparser.pipelines.InstagramPhotosPipeline': 200,\n",
    "    'instaparser.pipelines.DataBasePipeline': 300,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
